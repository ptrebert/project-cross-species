{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing  G9930_Ennn_Tnnn_oviAri3_any.from.hg19.feat.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/TL/epigenetics2/work/pebert/conda/envs/py3/lib/python3.5/site-packages/tables/path.py:118: NaturalNameWarning: object name is a Python keyword: 'from'; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  % (name, _warnInfo), NaturalNameWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing  G9930_Ennn_Tnnn_bosTau7_any.from.hg19.feat.h5\n",
      "Writing  G9930_Ennn_Tnnn_equCab2_any.from.mm9.feat.h5\n",
      "Writing  G9930_Ennn_Tnnn_rn5_any.from.mm9.feat.h5\n",
      "Writing  G9930_Ennn_Tnnn_galGal3_any.from.hg19.feat.h5\n",
      "Writing  G9930_Ennn_Tnnn_hg19_any.from.mm9.feat.h5\n",
      "Writing  G9930_Ennn_Tnnn_rheMac2_any.from.hg19.feat.h5\n",
      "Writing  G9930_Ennn_Tnnn_rn5_any.from.hg19.feat.h5\n",
      "Writing  G9930_Ennn_Tnnn_oryCun2_any.from.mm9.feat.h5\n",
      "Writing  G9930_Ennn_Tnnn_rheMac2_any.from.mm9.feat.h5\n",
      "Writing  G9930_Ennn_Tnnn_oryCun2_any.from.hg19.feat.h5\n",
      "Writing  G9930_Ennn_Tnnn_canFam3_any.from.hg19.feat.h5\n",
      "Writing  G9930_Ennn_Tnnn_susScr2_any.from.mm9.feat.h5\n",
      "Writing  G9930_Ennn_Tnnn_mm9_any.from.hg19.feat.h5\n",
      "Writing  G9930_Ennn_Tnnn_galGal3_any.from.mm9.feat.h5\n",
      "Writing  G9930_Ennn_Tnnn_bosTau7_any.from.mm9.feat.h5\n",
      "Writing  G9930_Ennn_Tnnn_susScr2_any.from.hg19.feat.h5\n",
      "Writing  G9930_Ennn_Tnnn_equCab2_any.from.hg19.feat.h5\n",
      "Writing  G9930_Ennn_Tnnn_felCat5_any.from.hg19.feat.h5\n",
      "Writing  G9930_Ennn_Tnnn_monDom5_any.from.hg19.feat.h5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os as os\n",
    "import sys as sys\n",
    "import collections as col\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 2018-04-22\n",
    "# What does this do?\n",
    "# After all tissue-specific train and test datasets\n",
    "# have been generated by the main pipeline, this\n",
    "# noteboook creates averaged datasets as requested\n",
    "# by Christoph. These averaged datasets can be used\n",
    "# train generic classifiers (i.e., classifiers that\n",
    "# do not just use the epigenetic signal of one cell type)\n",
    "\n",
    "create_traindata = False\n",
    "create_testdata = True\n",
    "\n",
    "fhgfs_base = '/TL/deep/fhgfs/projects/pebert/thesis/projects/cross_species/processing/norm'\n",
    "train_root = os.path.join(fhgfs_base, 'task_traindata_exp/train_datasets')\n",
    "test_root = os.path.join(fhgfs_base, 'task_testdata_exp/test_datasets')\n",
    "\n",
    "drop_cols = ['score_body', 'strand_body',\n",
    "             'score_uprr', 'strand_uprr',\n",
    "             'score_reg5p', 'strand_reg5p']\n",
    "\n",
    "\n",
    "def create_avg_traindata():\n",
    "    pairs = col.defaultdict(list)\n",
    "    for root, dirs, featfiles in os.walk(train_root):\n",
    "        for ff in featfiles:\n",
    "            if ff.endswith('.feat.h5'):\n",
    "                pairs[root].append(ff)\n",
    "\n",
    "    for folder, featfiles in pairs.items():\n",
    "        epi_done = set()\n",
    "        trans_done = set()\n",
    "        md = None\n",
    "        used_files = []\n",
    "        epi_data = col.defaultdict(list)\n",
    "        trans_data = col.defaultdict(list)\n",
    "        for ff in featfiles:\n",
    "            if ff.startswith('G99'):\n",
    "                continue\n",
    "            _, epi, trans, _ = ff.split('_', 3)\n",
    "            if epi in epi_done and trans in trans_done:\n",
    "                continue\n",
    "            process_splits = False\n",
    "            fpath = os.path.join(folder, ff)\n",
    "            with pd.HDFStore(fpath, 'r') as hdf:\n",
    "                md = hdf['metadata']\n",
    "                for k in hdf.keys():\n",
    "                    if k.startswith('/metadata'):\n",
    "                        continue\n",
    "                    data = hdf[k]\n",
    "                    chrom = os.path.split(k)[-1]\n",
    "                    if epi not in epi_done or process_splits:\n",
    "                        used_files.append(ff)\n",
    "                        data.drop(drop_cols, axis=1, inplace=True)\n",
    "                        epi_data[chrom].append(data.copy())\n",
    "                        epi_done.add(epi)\n",
    "                        process_splits = True\n",
    "                    if trans not in trans_done or process_splits:\n",
    "                        data = data.loc[:, ['name', 'symbol', 'tpm_norm', 'rank_norm']]\n",
    "                        trans_data[chrom].append(data.copy())\n",
    "                        trans_done.add(trans)\n",
    "                        process_splits = True\n",
    "        print('Generating training data ', os.path.split(folder)[-1])\n",
    "        group_id = md.loc[0, 'group'].split('/')[4]\n",
    "        md['group'] = md['group'].str.replace(group_id, 'G9930')\n",
    "        hdf_path = os.path.split(md.loc[0, 'group'])[0]\n",
    "        md['srcfile'] = ','.join(used_files)\n",
    "        prefix, suffix = featfiles[-1].split('.', 1)\n",
    "        ref = prefix.split('_')[3]\n",
    "        outfile = 'G9930_Ennn_Tnnn_' + ref + '_any.' + suffix\n",
    "        print('File ', outfile)\n",
    "        outpath = os.path.join(folder, outfile)        \n",
    "        with pd.HDFStore(outpath, 'w', complib='blosc', complevel=9) as hdf:\n",
    "            hdf.put('metadata', md, format='table')\n",
    "            for chrom, epigenomes in epi_data.items():\n",
    "                chrom_epi = pd.concat(epigenomes, axis=0, ignore_index=False)\n",
    "                chrom_epi.drop(['tpm_norm', 'rank_norm'], axis=1, inplace=True)\n",
    "                chrom_epi = chrom_epi.groupby(['name', 'symbol']).mean()\n",
    "                chrom_epi['name'] = chrom_epi.index.get_level_values('name').values\n",
    "                chrom_epi['symbol'] = chrom_epi.index.get_level_values('symbol').values\n",
    "                chrom_epi.reset_index(drop=True, inplace=True)\n",
    "\n",
    "                chrom_trans = pd.concat(trans_data[chrom], axis=0, ignore_index=False)\n",
    "                chrom_trans.reset_index(drop=True, inplace=True)\n",
    "                chrom_trans = chrom_trans.groupby(['name', 'symbol']).mean()\n",
    "                chrom_trans['name'] = chrom_trans.index.get_level_values('name').values\n",
    "                chrom_trans['symbol'] = chrom_trans.index.get_level_values('symbol').values\n",
    "                chrom_data = chrom_epi.merge(chrom_trans, on=['name', 'symbol'], how='outer')\n",
    "\n",
    "                chrom_path = os.path.join(hdf_path, chrom)\n",
    "\n",
    "                hdf.put(chrom_path, chrom_data, format='fixed')\n",
    "        print('====')\n",
    "    return True\n",
    "\n",
    "def create_avg_testdata():\n",
    "    pairs = col.defaultdict(list)\n",
    "    for root, dirs, featfiles in os.walk(test_root):\n",
    "        for ff in featfiles:\n",
    "            if ff.endswith('.feat.h5'):\n",
    "                pairs[root].append(ff)\n",
    "\n",
    "    for folder, featfiles in pairs.items():\n",
    "        epi_done = set()\n",
    "        trans_done = set()\n",
    "        md = None\n",
    "        used_files = []\n",
    "        epi_data = col.defaultdict(list)\n",
    "        trans_data = col.defaultdict(list)\n",
    "        for ff in featfiles:\n",
    "            if ff.startswith('G99'):\n",
    "                continue\n",
    "            _, epi, trans, _ = ff.split('_', 3)\n",
    "            if epi in epi_done and trans in trans_done:\n",
    "                continue\n",
    "            process_splits = False\n",
    "            fpath = os.path.join(folder, ff)\n",
    "            with pd.HDFStore(fpath, 'r') as hdf:\n",
    "                md = hdf['metadata']\n",
    "                for k in hdf.keys():\n",
    "                    if k.startswith('/metadata'):\n",
    "                        continue\n",
    "                    data = hdf[k]\n",
    "                    chrom = os.path.split(k)[-1]\n",
    "                    if epi not in epi_done or process_splits:\n",
    "                        used_files.append(ff)\n",
    "                        data.drop(drop_cols, axis=1, inplace=True)\n",
    "                        epi_data[chrom].append(data.copy())\n",
    "                        epi_done.add(epi)\n",
    "                        process_splits = True\n",
    "                    if trans not in trans_done or process_splits:\n",
    "                        data = data.loc[:, ['name', 'symbol', 'tpm_norm', 'rank_norm']]\n",
    "                        trans_data[chrom].append(data.copy())\n",
    "                        trans_done.add(trans)\n",
    "                        process_splits = True\n",
    "\n",
    "        group_id = md.loc[0, 'group'].split('/')[4]\n",
    "        md['group'] = md['group'].str.replace(group_id, 'G9930')\n",
    "        hdf_path = os.path.split(md.loc[0, 'group'])[0]\n",
    "        md['srcfile'] = ','.join(used_files)\n",
    "        prefix, suffix = featfiles[-1].split('.', 1)\n",
    "        ref = prefix.split('_')[3]\n",
    "        outfile = 'G9930_Ennn_Tnnn_' + ref + '_any.' + suffix\n",
    "        outpath = os.path.join(folder, outfile)\n",
    "        \n",
    "        print('Writing ', outfile)\n",
    "        with pd.HDFStore(outpath, 'w', complib='blosc', complevel=9) as hdf:\n",
    "            hdf.put('metadata', md, format='table')\n",
    "            for chrom, epigenomes in epi_data.items():\n",
    "                chrom_epi = pd.concat(epigenomes, axis=0, ignore_index=False)\n",
    "                chrom_epi.drop(['tpm_norm', 'rank_norm'], axis=1, inplace=True)\n",
    "                chrom_epi = chrom_epi.groupby(['name', 'symbol']).mean()\n",
    "                chrom_epi['name'] = chrom_epi.index.get_level_values('name').values\n",
    "                chrom_epi['symbol'] = chrom_epi.index.get_level_values('symbol').values\n",
    "                chrom_epi.reset_index(drop=True, inplace=True)\n",
    "\n",
    "                chrom_trans = pd.concat(trans_data[chrom], axis=0, ignore_index=False)\n",
    "                chrom_trans.reset_index(drop=True, inplace=True)\n",
    "                chrom_trans = chrom_trans.groupby(['name', 'symbol']).mean()\n",
    "                chrom_trans['name'] = chrom_trans.index.get_level_values('name').values\n",
    "                chrom_trans['symbol'] = chrom_trans.index.get_level_values('symbol').values\n",
    "                chrom_trans.reset_index(drop=True, inplace=True)\n",
    "\n",
    "                chrom_data = chrom_epi.merge(chrom_trans, on=['name', 'symbol'], how='outer')\n",
    "                chrom_path = os.path.join(hdf_path, chrom)\n",
    "\n",
    "                hdf.put(chrom_path, chrom_data, format='fixed')\n",
    "    return True\n",
    "\n",
    "if create_traindata:\n",
    "    _ = create_avg_traindata()\n",
    "\n",
    "if create_testdata:\n",
    "    _ = create_avg_testdata()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
